import gym
import tensorflow as tf
import numpy as np
import minerl
import collections


class DDDQN(tf.keras.Model):
    def __init__(self):
        super(DDDQN, self).__init__()
        self.d1 = tf.keras.layers.Dense(256, activation=tf.nn.relu)
        self.d2 = tf.keras.layers.Dense(256, activation=tf.nn.relu)
        self.v = tf.keras.layers.Dense(1, activation=None)
        self.a = tf.keras.layers.Dense(len(list(env.action_space)), activation=None)

    def call(self, input):
        x = self.d1(input)
        x = self.d2(x)
        v = self.v(x)
        a = self.a(x)
        q_val = v + (a - tf.math.reduce_mean(a, axis=1, keepdims=True))
        return q_val

class exp_replay():
    def __init__(self, buffer_size= 1000):
        self.buffer_size = buffer_size
        self.state_mem = np.zeros((self.buffer_size, 12289), dtype=collections.OrderedDict)
        self.action_mem = np.zeros((self.buffer_size), dtype=collections.OrderedDict)
        self.reward_mem = np.zeros((self.buffer_size), dtype=np.float32)
        self.next_state_mem = np.zeros((self.buffer_size, 12289), dtype=collections.OrderedDict)
        self.pointer = 0
        self.done_mem = np.zeros((self.buffer_size), dtype=np.bool)

    def add_exp(self, state, action, reward, next_state, done):
        idx = self.pointer % self.buffer_size
        self.state_mem[idx] = state
        self.action_mem[idx] = action
        self.reward_mem[idx] = reward
        self.next_state_mem[idx] = next_state
        self.done_mem[idx] = 1 - int(done)
        self.pointer += 1

    def sample_exp(self, batch_size= 32):
        max_mem = min(self.pointer, self.buffer_size)
        batch = np.random.choice(max_mem, batch_size, replace=False)
        states = self.state_mem[batch]
        actions = self.action_mem[batch]
        rewards = self.reward_mem[batch]
        next_states = self.next_state_mem[batch]
        dones = self.done_mem[batch]
        return states, actions, rewards, next_states, dones


class agent():
    def __init__(self, gamma=0.99, replace=100, lr=0.001):
        self.gamma = gamma
        self.epsilon = 1.0
        self.min_epsilon = 0.01
        self.epsilon_decay = 1e-3
        self.replace = replace
        self.trainstep = 0
        self.memory = exp_replay()
        self.batch_size = 512
        self.q_net = DDDQN()
        self.target_net = DDDQN()
        opt = tf.keras.optimizers.Adam(learning_rate=lr)
        self.q_net.compile(loss='mse', optimizer=opt)
        self.target_net.compile(loss='mse', optimizer=opt)

    def act(self, state):
        if np.random.rand() <= self.epsilon:
            return np.random.choice([i for i in range(len(list(env.action_space)))])

        else:
            actions = self.q_net.advantage(np.array([state]))
            action = actions
            return action

    def update_mem(self, state, action, reward, next_state, done):
        self.memory.add_exp(state, action, reward, next_state, done)

    def update_target(self):
        self.target_net.set_weights(self.q_net.get_weights())

    def update_epsilon(self):
        self.epsilon = self.epsilon - self.epsilon_decay if self.epsilon > self.min_epsilon else self.min_epsilon
        return self.epsilon

    def train(self):
        if self.memory.pointer < self.batch_size:
            return

        if self.trainstep % self.replace == 0:
            self.update_target()
        states, actions, rewards, next_states, dones = self.memory.sample_exp(self.batch_size)
        states = np.asarray(states)
        target = self.q_net.predict(states)
        next_state_val = self.target_net.predict(next_states)
        max_action = np.argmax(self.q_net.predict(next_states), axis=1)
        batch_index = np.arange(self.batch_size, dtype=np.int32)
        q_target = np.copy(target)  # optional
        q_target[batch_index, actions] = rewards + self.gamma * next_state_val[batch_index, max_action] * dones
        self.q_net.train_on_batch(states, q_target)
        self.update_epsilon()
        self.trainstep += 1

def converter(observation):

    obs = observation['pov']
    obs = np.ravel(obs)
    obs = obs / 255
    compass_angle = observation['compassAngle']

    compass_angle_scale = 180
    compass_scaled = compass_angle / compass_angle_scale
    compass_channel = np.ones(shape=list(obs.shape[:-1]) + [1], dtype=obs.dtype) * compass_scaled

    obs = np.concatenate([obs, compass_channel], axis=-1)

    nop = 0

    return obs

if __name__ == '__main__':
    env = gym.make('MineRLNavigateDense-v0')

    agentoo7 = agent()
    steps = 20

    for s in range(steps):
        state = env.reset()
        done = False
        net_reward = 0
        state = converter(state)
        while not done:
            env.render()
            action_index = agentoo7.act(state)
            action = env.action_space.noop()
            if (action_index == 0):
                action['attack'] = 1
            elif (action_index == 1):
                action['back'] = 1
            elif (action_index == 3):
                action['forward'] = 1
            elif (action_index == 4):
                action['jump'] = 1
            elif (action_index == 5):
                action['left'] = 1
                action['camera'] = [0, -5]
            elif (action_index == 7):
                action['right'] = 1
                action['camera'] = [0, 5]
            elif (action_index == 8):
                action['sneak'] = 1
            elif (action_index == 9):
                action['sprint'] = 1

            action['jump'] = 1
            action['attack'] = 1
            action['sprint'] = 0

            next_state, reward, done, info = env.step(action)
            next_state = converter(next_state)
            #agentoo7.update_mem(state, action, reward, next_state, done)
            #agentoo7.train()
            state = next_state
            net_reward += reward

            if done:
                print("total reward after {} episodes is {} and epsilon is {}".format(s, net_reward, agentoo7.epsilon))
